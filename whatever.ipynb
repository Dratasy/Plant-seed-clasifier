{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Side stuffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inlcude standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import urllib.request\n",
    "from functools import partial\n",
    "from urllib.error import HTTPError\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# PyTorch Lightning\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import seaborn as sns\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from PIL import Image\n",
    "\n",
    "# Libraries for ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from torchvision.transforms import v2 as tforms\n",
    "import torchmetrics\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths, seed for random stuffs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "TEST_DATASET_PATH = os.environ.get(\"PATH_TEST_DATASETS\", \"/kaggle/input/plant-seedlings-classification/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"/kaggle/input/plant-seedlings-classification\")\n",
    "TRAIN_DATASET_PATH = os.environ.get(\"PATH_TRAIN_DATASETS\", \"/kaggle/input/plant-seedlings-classification/train\")\n",
    "TEST_DATASET_PATH = os.environ.get(\"PATH_TEST_DATASETS\", \"/kaggle/input/plant-seedlings-classification/test\")\n",
    "\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"/kaggle/working\")\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "num_workers = os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"/kaggle/input/plant-seedlings-classification\")\n",
    "TRAIN_DATASET_PATH = os.environ.get(\"PATH_TRAIN_DATASETS\", \"/kaggle/input/plant-seedlings-classification/train\")\n",
    "TEST_DATASET_PATH = os.environ.get(\"PATH_TEST_DATASETS\", \"/kaggle/input/plant-seedlings-classification/test\")\n",
    "\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"./saved_models/\")\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "num_workers = os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Repo path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"./data\")\n",
    "TRAIN_DATASET_PATH = os.environ.get(\"PATH_TRAIN_DATASETS\", \"./data/train\")\n",
    "TEST_DATASET_PATH = os.environ.get(\"PATH_TEST_DATASETS\", \"./data/test\")\n",
    "\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"./saved_models/\")\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "num_workers = int(os.cpu_count()/ 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.set_cmap(\"cividis\")\n",
    "%matplotlib inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_orig()\n",
    "\n",
    "# Setting the seed\n",
    "seed = 13\n",
    "L.seed_everything(seed)\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\\\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do data engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fname_class_df(train_dir):\n",
    "    classes = os.listdir(train_dir)\n",
    "\n",
    "    file_lst = []\n",
    "    class_lst = []\n",
    "    class_idx_lst = []\n",
    "    for i, cl in enumerate(classes):\n",
    "        path = train_dir + f\"/{cl}\"\n",
    "        file_lst = file_lst + os.listdir(path)\n",
    "        class_lst = class_lst + [cl]* len(os.listdir(path))\n",
    "        class_idx_lst = class_idx_lst + [i]* len(os.listdir(path))\n",
    "    full_df = pd.DataFrame({\"file\": file_lst, \"class\": class_lst,\\\n",
    "                              \"class_idx\": class_idx_lst})\n",
    "    return full_df\n",
    "\n",
    "full_df = make_fname_class_df(TRAIN_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "g = sns.countplot(data=full_df, x=\"class\", order=full_df['class'].value_counts().index, palette='Greens_r')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_input_images(root_dir, images_per_row=7):\n",
    "    classes = os.listdir(root_dir)\n",
    "    fig = plt.figure(1, figsize=(len(classes)*3, images_per_row*3))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(len(classes), images_per_row), axes_pad=0.05)\n",
    "    for (row, class_name) in enumerate(classes):\n",
    "        path = root_dir + f\"/{class_name}\"\n",
    "        image_fname_samples = random.sample(os.listdir(path),k=images_per_row)\n",
    "        for img_fname, col in zip(image_fname_samples, range(images_per_row)):\n",
    "            img_path = root_dir + f\"/{class_name}\" + f\"/{img_fname}\"\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((224, 224))\n",
    "            grid[row*images_per_row + col].imshow(img)\n",
    "            grid[row*images_per_row + col].set_axis_off()\n",
    "            if col == images_per_row - 1:\n",
    "                grid[row*images_per_row + col].text(250, 110, class_name, verticalalignment='bottom', horizontalalignment=\"left\")\n",
    "\n",
    "visualize_input_images(TRAIN_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_input_images_2(root_dir, images_per_row=5):\n",
    "    classes = os.listdir(root_dir)\n",
    "    fig, axes = plt.subplots(nrows=len(classes), ncols=images_per_row, figsize=(15, 10))\n",
    "    fig.tight_layout(pad=0.0)\n",
    "    for (row, class_name) in enumerate(classes):\n",
    "        path = root_dir + f\"/{class_name}\"\n",
    "        image_fname_samples = random.sample(os.listdir(path),k=images_per_row)\n",
    "        for img_fname, col in zip(image_fname_samples, range(images_per_row)):\n",
    "            img_path = root_dir + f\"/{class_name}\" + f\"/{img_fname}\"\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((224, 224))\n",
    "            axes[row][col].imshow(img)\n",
    "            axes[row][col].set_axis_off()\n",
    "            if col == images_per_row - 1:\n",
    "                axes[row][col].text(250, 110, class_name, verticalalignment='bottom', horizontalalignment=\"left\")\n",
    "\n",
    "visualize_input_images_2(TRAIN_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "class PlantTrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.fname_class_df = make_fname_class_df(root_dir)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    # def class_to_idx(self):\n",
    "    #     return {_class: i for i, _class in enumerate(self.classes)}\n",
    "\n",
    "    # def idx_to_class(self):\n",
    "    #     return dict(zip(range(len(self.classes)), self.classes))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.fname_class_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.root_dir + f\"/{self.fname_class_df.loc[self.fname_class_df.index[idx], 'class']}\" \\\n",
    "            + f\"/{self.fname_class_df.loc[self.fname_class_df.index[idx], 'file']}\"\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        target = self.fname_class_df.loc[self.fname_class_df.index[idx], \"class_idx\"]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        return image, target\n",
    "\n",
    "class PlantTestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.fname_class_df = pd.DataFrame({\"file\": os.listdir(root_dir)})\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.fname_class_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.root_dir + f\"/{self.fname_class_df.loc[self.fname_class_df.index[idx], 'file']}\"\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(dataset, batch_size=8):\n",
    "    #imgage values in [0, 255]\n",
    "    rgb_values = torch.cat([img.reshape(3, -1) for img, target in dataset], dim=-1)\n",
    "    rgb_values_chunks = rgb_values.split(224*224*batch_size, dim=1)\n",
    "    rgb_mean_chunks = [torch.mean(chunk.float(), dim=-1) for chunk in rgb_values_chunks] #list of tensor shape (3,)\n",
    "    rgb_mean = torch.mean(torch.stack(rgb_mean_chunks, dim=0), dim=0).reshape((3, 1)) #shape (3, 1)\n",
    "\n",
    "    rgb_var_chunks = [torch.mean((chunk - rgb_mean).float()** 2, dim=-1) for chunk in rgb_values_chunks]\n",
    "    rgb_std = torch.sqrt(torch.mean(torch.stack(rgb_var_chunks, dim=0), dim=0))\n",
    "\n",
    "    return rgb_mean.squeeze()/ 255.0, rgb_std/ 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDataModule(L.LightningDataModule):\n",
    "    def __init__(self, root_dir, test_dir, batch_size):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.test_dir = test_dir\n",
    "        self.batch_size = batch_size\n",
    "        backbone_mean, backbone_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "        self.transform = tforms.Compose([\n",
    "            tforms.ToImage(),\n",
    "            tforms.ToDtype(torch.uint8, scale=True),\n",
    "            tforms.RandomAffine(degrees=10, translate=(0.2, 0.2)),\n",
    "            tforms.RandomHorizontalFlip(),\n",
    "            tforms.RandomVerticalFlip(),\n",
    "            tforms.Resize((224, 224), antialias=True),\n",
    "            tforms.ToDtype(torch.float32, scale=True),\n",
    "            tforms.Normalize(backbone_mean, backbone_std)\n",
    "        ])\n",
    "        self.test_transform = tforms.Compose([\n",
    "            tforms.ToImage(),\n",
    "            tforms.ToDtype(torch.uint8, scale=True),\n",
    "            tforms.Resize((224, 224), antialias=True),\n",
    "            tforms.ToDtype(torch.float32, scale=True),\n",
    "            tforms.Normalize(backbone_mean, backbone_std)\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":\n",
    "            full_dataset = PlantTrainDataset(self.root_dir,transform=self.transform)\n",
    "            \n",
    "            self.train_dataset, self.val_dataset = random_split(full_dataset, [0.8, 0.2],\\\n",
    "                generator=torch.Generator().manual_seed(seed)\n",
    "            )\n",
    "        if stage == \"test\":\n",
    "            full_dataset = PlantTrainDataset(self.root_dir,transform=self.transform)\n",
    "            \n",
    "            _, self.test_dataset = random_split(full_dataset, [0.8, 0.2],\\\n",
    "                generator=torch.Generator().manual_seed(torch.initial_seed())\n",
    "            )\n",
    "        if stage == \"predict\":\n",
    "            self.predict_dataset = PlantTestDataset(self.test_dir, transform=self.test_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, drop_last=False,\\\n",
    "                    num_workers=num_workers, pin_memory=True, worker_init_fn=seed_worker,\\\n",
    "                    generator=torch.Generator().manual_seed(torch.initial_seed()), persistent_workers=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False,\\\n",
    "                    num_workers=num_workers, worker_init_fn=seed_worker,\\\n",
    "                    generator=torch.Generator().manual_seed(torch.initial_seed()), persistent_workers=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False,\\\n",
    "                    num_workers=num_workers, worker_init_fn=seed_worker,\\\n",
    "                    generator=torch.Generator().manual_seed(torch.initial_seed()), persistent_workers=True\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False,\\\n",
    "                    num_workers=num_workers, worker_init_fn=seed_worker,\\\n",
    "                    generator=torch.Generator().manual_seed(torch.initial_seed()), persistent_workers=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "class PlantSeedModule(L.LightningModule):\n",
    "    def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams, metrics_name, metrics_hparams):\n",
    "        super().__init__()\n",
    "        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n",
    "        self.save_hyperparameters()\n",
    "        # Create model\n",
    "        self.model = create_model(model_name, model_hparams)\n",
    "        # Create loss module\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "        # Example input for visualizing the graph in Tensorboard\n",
    "        self.example_input_array = torch.zeros((1, 3, 224, 224), dtype=torch.float32)\n",
    "\n",
    "        self._configure_metrics()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "        loss = self.loss_module(outputs, targets)\n",
    "        self.train_metrics.update(preds, targets)\n",
    "        # By default logs it per step and epoch\n",
    "        self.log(\"train_loss\", loss, logger=True)\n",
    "        self.log_dict(self.train_metrics, on_step=False, on_epoch=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "        self.valid_metrics.update(preds, targets)\n",
    "        # By default logs it per epoch\n",
    "        self.log_dict(self.valid_metrics, on_step=False, on_epoch=True, logger=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "        self.test_metrics.update(preds, targets)\n",
    "        # By default logs it per epoch\n",
    "        self.log_dict(self.test_metrics, on_step=False, on_epoch=True, logger=True)\n",
    "\n",
    "        # targets_np, preds_np = targets.cpu().data.numpy(), preds.cpu().data.numpy()\n",
    "        # print(f\"\\n#################################################\")\n",
    "        # print(f\"accuracy: {accuracy_score(targets_np, preds_np)}\")\n",
    "        # print(f\"precision: {precision_score(targets_np, preds_np, average='micro')}\")\n",
    "        # print(f\"recall: {recall_score(targets_np, preds_np, average='micro')}\")\n",
    "        # print(f\"f1: {f1_score(targets_np, preds_np, average='micro')}\")\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs = batch\n",
    "        outputs = self(inputs)\n",
    "        return torch.argmax(outputs, dim=-1)\n",
    "\n",
    "    def _configure_metrics(self):\n",
    "        metric_lst = []\n",
    "        for metric_name, metric_hprams in zip(self.hparams.metrics_name, self.hparams.metrics_hparams):\n",
    "            if metric_name == \"Accuracy\":\n",
    "                metric_lst.append(torchmetrics.Accuracy(**metric_hprams))\n",
    "            elif metric_name == \"Precision\":\n",
    "                metric_lst.append(torchmetrics.Precision(**metric_hprams))\n",
    "            elif metric_name == \"Recall\":\n",
    "                metric_lst.append(torchmetrics.Recall(**metric_hprams))\n",
    "            elif metric_name == \"F1Score\":\n",
    "                metric_lst.append(torchmetrics.F1Score(**metric_hprams))\n",
    "            else:\n",
    "                pass\n",
    "        metrics = torchmetrics.MetricCollection(metric_lst)\n",
    "        self.train_metrics = metrics.clone(prefix='train_')\n",
    "        self.valid_metrics = metrics.clone(prefix='val_')\n",
    "        self.test_metrics = metrics.clone(prefix='test_')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams.optimizer_name == \"Adam\":\n",
    "            # AdamW is Adam with a correct implementation of weight decay (see here\n",
    "            # for details: https://arxiv.org/pdf/1711.05101.pdf)\n",
    "            optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), **self.hparams.optimizer_hparams)\n",
    "        elif self.hparams.optimizer_name == \"SGD\":\n",
    "            optimizer = optim.SGD(filter(lambda p: p.requires_grad, self.model.parameters()), **self.hparams.optimizer_hparams)\n",
    "        else:\n",
    "            assert False, f'Unknown optimizer: \"{self.hparams.optimizer_name}\"'\n",
    "\n",
    "        # We will reduce the learning rate by 0.1 after 100 and 150 epochs\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "\n",
    "def create_model(model_name, model_hparams):\n",
    "    if model_name in model_dict:\n",
    "        return model_dict[model_name](**model_hparams)\n",
    "    else:\n",
    "        assert False, f'Unknown model name \"{model_name}\". Available models are: {str(model_dict.keys())}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearningEfficientnet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.hparams = SimpleNamespace(num_classes=num_classes)\n",
    "        self._create_network()\n",
    "        self._init_params()\n",
    "\n",
    "    def _create_network(self):\n",
    "        backbone = torchvision.models.efficientnet_b4(weights='IMAGENET1K_V1')\n",
    "        self.features = nn.Sequential(*(list(backbone.children())[:-1]))\n",
    "        num_ftrs = backbone.classifier[1].in_features\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(num_ftrs, 128),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Dropout(0.2),\n",
    "                            torch.nn.Linear(128, self.hparams.num_classes)\n",
    "                        )\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def _init_params(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        ftrs = self.features(x)\n",
    "        ftrs = ftrs.view(ftrs.size(0), -1)\n",
    "        y = self.classifier(ftrs)\n",
    "        return y\n",
    "\n",
    "model_dict[\"TLEfficientnet\"] = TransferLearningEfficientnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, datamodule, max_epochs, save_name=None, **kwargs):\n",
    "    \"\"\"Train model.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
    "        save_name (optional): If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    \"\"\"\n",
    "    if save_name is None:\n",
    "        save_name = model_name\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),  # Where to save models\n",
    "        # We run on a single GPU (if possible)\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        # How many epochs to train for if no patience is set\n",
    "        max_epochs=max_epochs,\n",
    "        logger=TensorBoardLogger(save_dir=os.path.join(CHECKPOINT_PATH, save_name), name=\"logs\", version=\"version_1\"),\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(\n",
    "                #dirpath=os.path.join(CHECKPOINT_PATH, save_name),\n",
    "                mode=\"max\", monitor=\"val_MulticlassAccuracy\", verbose=True, save_last=True, save_top_k=2, enable_version_counter=True\n",
    "            ),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],  # Log learning rate every epoch\n",
    "        log_every_n_steps=50,\n",
    "        # profiler=\"simple\"\n",
    "    )  # In case your notebook crashes due to the progress bar, consider increasing the refresh rate\n",
    "    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + \".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        # Automatically loads the model with the saved hyperparameters\n",
    "        model = PlantSeedModule.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        L.seed_everything(seed)  # To be reproducible\n",
    "        model = PlantSeedModule(model_name=model_name, **kwargs)\n",
    "        trainer.fit(model, datamodule=datamodule, ckpt_path=\"last\")\n",
    "        model = PlantSeedModule.load_from_checkpoint(\n",
    "            trainer.checkpoint_callback.best_model_path\n",
    "        )  # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    test_result = trainer.test(model, datamodule=datamodule, verbose=True)\n",
    "    #result = {\"test\": test_result[0][\"test_Accuracy\"]}\n",
    "\n",
    "    return trainer, model, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "plantdata = PlantDataModule(TRAIN_DATASET_PATH, TEST_DATASET_PATH, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer, model, test_result = train_model(\n",
    "    model_name= \"TLEfficientnet\",\n",
    "    datamodule= plantdata,\n",
    "    max_epochs=10,\n",
    "    model_hparams={\"num_classes\": 12},\n",
    "    optimizer_name=\"Adam\",\n",
    "    optimizer_hparams={\"lr\": 1e-3, \"weight_decay\": 1e-4},\n",
    "    metrics_name= [\"Accuracy\", \"Precision\", \"Recall\", \"F1Score\"],\n",
    "    metrics_hparams= [\n",
    "        {\"task\": \"multiclass\", \"num_classes\": 12, \"average\": \"macro\"},\n",
    "        {\"task\": \"multiclass\", \"num_classes\": 12, \"average\": \"macro\"},\n",
    "        {\"task\": \"multiclass\", \"num_classes\": 12, \"average\": \"macro\"},\n",
    "        {\"task\": \"multiclass\", \"num_classes\": 12, \"average\": \"macro\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /saved_models/TLEfficientnet/logs/\n",
    "\n",
    "import IPython\n",
    "\n",
    "display(IPython.display.HTML('''\n",
    "<button id='open_tb'>Open TensorBoard</button>\n",
    "<button id='hide_tb'>Hide TensorBoard</button>\n",
    "<script>document.querySelector('#open_tb').onclick = () => { window.open(document.querySelector('iframe').src, \"__blank\") }\n",
    "        document.querySelector('#hide_tb').onclick = () => { document.querySelector('iframe').style.display = \"none\" }</script>'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(model, plantdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(TRAIN_DATASET_PATH)\n",
    "idx_to_class_dict = {i: _class for i, _class in enumerate(classes)}\n",
    "species = np.vectorize(lambda idx: idx_to_class_dict[idx])(torch.cat(preds, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "TEST_DATASET_PATH = os.environ.get(\"PATH_TEST_DATASETS\", \"/kaggle/input/plant-seedlings-classification/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = [\"Charklock\"] * len(os.listdir(TEST_DATASET_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"file\": os.listdir(TEST_DATASET_PATH),\n",
    "                         \"species\": species})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this cell if using Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
    "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tempfile import NamedTemporaryFile\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import unquote, urlparse\n",
    "from urllib.error import HTTPError\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "CHUNK_SIZE = 40960\n",
    "DATA_SOURCE_MAPPING = 'plant-seedlings-classification:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F7880%2F862031%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240422%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240422T012524Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dc72d44995974dac6a31ba7b227aa146caf438595f804ac4af3b7d4eb91bc66007d6afcf8d84b7216fb06823597f09a8f3be99f4c906b50a701a792d56722c6183fadeaa07db5542e259b2cbf0379e13d4a0b42b2f66cf15308562a94b80f90d2b42c7a6828fe08f5126cf8bf8ed5ec15979c0ef6fa65e6eb42dd77ec71231487a6f7f37e06f16371b43f340f3ae7f7904a6f417f40be3a14b41e489a1e90905d1d14e71745308f658768e061189b61d0edc2c6d72be2d1f86981109cd0af7dee0d6662d185f709d0a31876da4ccab39447251888c15ebe2e588f126bfe516609f03f27f7822ef6cd9b1c079f5494046d71da57cf48a1d0224a97b2eee3cc0867'\n",
    "\n",
    "KAGGLE_INPUT_PATH='/kaggle/input'\n",
    "KAGGLE_WORKING_PATH='/kaggle/working'\n",
    "KAGGLE_SYMLINK='kaggle'\n",
    "\n",
    "!umount /kaggle/input/ 2> /dev/null\n",
    "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
    "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
    "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
    "\n",
    "try:\n",
    "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
    "except FileExistsError:\n",
    "  pass\n",
    "try:\n",
    "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
    "except FileExistsError:\n",
    "  pass\n",
    "\n",
    "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
    "    directory, download_url_encoded = data_source_mapping.split(':')\n",
    "    download_url = unquote(download_url_encoded)\n",
    "    filename = urlparse(download_url).path\n",
    "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
    "    try:\n",
    "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
    "            total_length = fileres.headers['content-length']\n",
    "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
    "            dl = 0\n",
    "            data = fileres.read(CHUNK_SIZE)\n",
    "            while len(data) > 0:\n",
    "                dl += len(data)\n",
    "                tfile.write(data)\n",
    "                done = int(50 * dl / int(total_length))\n",
    "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
    "                sys.stdout.flush()\n",
    "                data = fileres.read(CHUNK_SIZE)\n",
    "            if filename.endswith('.zip'):\n",
    "              with ZipFile(tfile) as zfile:\n",
    "                zfile.extractall(destination_path)\n",
    "            else:\n",
    "              with tarfile.open(tfile.name) as tarfile:\n",
    "                tarfile.extractall(destination_path)\n",
    "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
    "    except HTTPError as e:\n",
    "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
    "        continue\n",
    "    except OSError as e:\n",
    "        print(f'Failed to load {download_url} to path {destination_path}')\n",
    "        continue\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle first cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    #for filename in filenames:\n",
    "        #print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
