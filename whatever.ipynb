{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inlcude standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import urllib.request\n",
    "from functools import partial\n",
    "from urllib.error import HTTPError\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# PyTorch Lightning\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import seaborn as sns\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from PIL import Image\n",
    "\n",
    "# Libraries for ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from torchvision.transforms import v2 as tforms\n",
    "import torchmetrics\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths, seed for random stuffs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "TEST_DATASET_PATH = os.environ.get(\"PATH_TEST_DATASETS\", \"/kaggle/input/plant-seedlings-classification/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"/kaggle/input/plant-seedlings-classification\")\n",
    "TRAIN_DATASET_PATH = os.environ.get(\"PATH_TRAIN_DATASETS\", \"/kaggle/input/plant-seedlings-classification/train\")\n",
    "TEST_DATASET_PATH = os.environ.get(\"PATH_TEST_DATASETS\", \"/kaggle/input/plant-seedlings-classification/test\")\n",
    "\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"/kaggle/working\")\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "num_workers = os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"/kaggle/input/plant-seedlings-classification\")\n",
    "TRAIN_DATASET_PATH = os.environ.get(\"PATH_TRAIN_DATASETS\", \"/kaggle/input/plant-seedlings-classification/train\")\n",
    "TEST_DATASET_PATH = os.environ.get(\"PATH_TEST_DATASETS\", \"/kaggle/input/plant-seedlings-classification/test\")\n",
    "\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"./saved_models/\")\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "num_workers = os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Repo path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"./data\")\n",
    "TRAIN_DATASET_PATH = os.environ.get(\"PATH_TRAIN_DATASETS\", \"./data/train\")\n",
    "TEST_DATASET_PATH = os.environ.get(\"PATH_TEST_DATASETS\", \"./data/test\")\n",
    "\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"./saved_models/\")\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "num_workers = int(os.cpu_count()/ 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.set_cmap(\"cividis\")\n",
    "%matplotlib inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_orig()\n",
    "\n",
    "# Setting the seed\n",
    "seed = 13\n",
    "L.seed_everything(seed)\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\\\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do data engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fname_class_df(train_dir):\n",
    "    classes = os.listdir(train_dir)\n",
    "\n",
    "    file_lst = []\n",
    "    class_lst = []\n",
    "    class_idx_lst = []\n",
    "    for i, cl in enumerate(classes):\n",
    "        path = train_dir + f\"/{cl}\"\n",
    "        file_lst = file_lst + os.listdir(path)\n",
    "        class_lst = class_lst + [cl]* len(os.listdir(path))\n",
    "        class_idx_lst = class_idx_lst + [i]* len(os.listdir(path))\n",
    "    full_df = pd.DataFrame({\"file\": file_lst, \"class\": class_lst,\\\n",
    "                              \"class_idx\": class_idx_lst})\n",
    "    return full_df\n",
    "\n",
    "full_df = make_fname_class_df(TRAIN_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "g = sns.countplot(data=full_df, x=\"class\", order=full_df['class'].value_counts().index, palette='Greens_r')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_input_images(root_dir, images_per_row=7):\n",
    "    classes = os.listdir(root_dir)\n",
    "    fig = plt.figure(1, figsize=(len(classes)*3, images_per_row*3))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(len(classes), images_per_row), axes_pad=0.05)\n",
    "    for (row, class_name) in enumerate(classes):\n",
    "        path = root_dir + f\"/{class_name}\"\n",
    "        image_fname_samples = random.sample(os.listdir(path),k=images_per_row)\n",
    "        for img_fname, col in zip(image_fname_samples, range(images_per_row)):\n",
    "            img_path = root_dir + f\"/{class_name}\" + f\"/{img_fname}\"\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((224, 224))\n",
    "            grid[row*images_per_row + col].imshow(img)\n",
    "            grid[row*images_per_row + col].set_axis_off()\n",
    "            if col == images_per_row - 1:\n",
    "                grid[row*images_per_row + col].text(250, 110, class_name, verticalalignment='bottom', horizontalalignment=\"left\")\n",
    "\n",
    "visualize_input_images(TRAIN_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_input_images_2(root_dir, images_per_row=5):\n",
    "    classes = os.listdir(root_dir)\n",
    "    fig, axes = plt.subplots(nrows=len(classes), ncols=images_per_row, figsize=(15, 10))\n",
    "    fig.tight_layout(pad=0.0)\n",
    "    for (row, class_name) in enumerate(classes):\n",
    "        path = root_dir + f\"/{class_name}\"\n",
    "        image_fname_samples = random.sample(os.listdir(path),k=images_per_row)\n",
    "        for img_fname, col in zip(image_fname_samples, range(images_per_row)):\n",
    "            img_path = root_dir + f\"/{class_name}\" + f\"/{img_fname}\"\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((224, 224))\n",
    "            axes[row][col].imshow(img)\n",
    "            axes[row][col].set_axis_off()\n",
    "            if col == images_per_row - 1:\n",
    "                axes[row][col].text(250, 110, class_name, verticalalignment='bottom', horizontalalignment=\"left\")\n",
    "\n",
    "visualize_input_images_2(TRAIN_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "class PlantTrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.fname_class_df = make_fname_class_df(root_dir)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    # def class_to_idx(self):\n",
    "    #     return {_class: i for i, _class in enumerate(self.classes)}\n",
    "\n",
    "    # def idx_to_class(self):\n",
    "    #     return dict(zip(range(len(self.classes)), self.classes))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.fname_class_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.root_dir + f\"/{self.fname_class_df.loc[self.fname_class_df.index[idx], 'class']}\" \\\n",
    "            + f\"/{self.fname_class_df.loc[self.fname_class_df.index[idx], 'file']}\"\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        target = self.fname_class_df.loc[self.fname_class_df.index[idx], \"class_idx\"]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        return image, target\n",
    "\n",
    "class PlantTestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.fname_class_df = pd.DataFrame({\"file\": os.listdir(root_dir)})\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.fname_class_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.root_dir + f\"/{self.fname_class_df.loc[self.fname_class_df.index[idx], 'file']}\"\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(dataset, batch_size=8):\n",
    "    #imgage values in [0, 255]\n",
    "    rgb_values = torch.cat([img.reshape(3, -1) for img, target in dataset], dim=-1)\n",
    "    rgb_values_chunks = rgb_values.split(224*224*batch_size, dim=1)\n",
    "    rgb_mean_chunks = [torch.mean(chunk.float(), dim=-1) for chunk in rgb_values_chunks] #list of tensor shape (3,)\n",
    "    rgb_mean = torch.mean(torch.stack(rgb_mean_chunks, dim=0), dim=0).reshape((3, 1)) #shape (3, 1)\n",
    "\n",
    "    rgb_var_chunks = [torch.mean((chunk - rgb_mean).float()** 2, dim=-1) for chunk in rgb_values_chunks]\n",
    "    rgb_std = torch.sqrt(torch.mean(torch.stack(rgb_var_chunks, dim=0), dim=0))\n",
    "\n",
    "    return rgb_mean.squeeze()/ 255.0, rgb_std/ 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDataModule(L.LightningDataModule):\n",
    "    def __init__(self, root_dir, test_dir, batch_size):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.test_dir = test_dir\n",
    "        self.batch_size = batch_size\n",
    "        backbone_mean, backbone_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "        self.transform = tforms.Compose([\n",
    "            tforms.ToImage(),\n",
    "            tforms.ToDtype(torch.uint8, scale=True),\n",
    "            tforms.RandomAffine(degrees=10, translate=(0.2, 0.2)),\n",
    "            tforms.RandomHorizontalFlip(),\n",
    "            tforms.RandomVerticalFlip(),\n",
    "            tforms.Resize((224, 224), antialias=True),\n",
    "            tforms.ToDtype(torch.float32, scale=True),\n",
    "            tforms.Normalize(backbone_mean, backbone_std)\n",
    "        ])\n",
    "        self.test_transform = tforms.Compose([\n",
    "            tforms.ToImage(),\n",
    "            tforms.ToDtype(torch.uint8, scale=True),\n",
    "            tforms.Resize((224, 224), antialias=True),\n",
    "            tforms.ToDtype(torch.float32, scale=True),\n",
    "            tforms.Normalize(backbone_mean, backbone_std)\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":\n",
    "            full_dataset = PlantTrainDataset(self.root_dir,transform=self.transform)\n",
    "            \n",
    "            self.train_dataset, self.val_dataset = random_split(full_dataset, [0.8, 0.2],\\\n",
    "                generator=torch.Generator().manual_seed(seed)\n",
    "            )\n",
    "        if stage == \"test\":\n",
    "            full_dataset = PlantTrainDataset(self.root_dir,transform=self.transform)\n",
    "            \n",
    "            _, self.test_dataset = random_split(full_dataset, [0.8, 0.2],\\\n",
    "                generator=torch.Generator().manual_seed(torch.initial_seed())\n",
    "            )\n",
    "        if stage == \"predict\":\n",
    "            self.predict_dataset = PlantTestDataset(self.test_dir, transform=self.test_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, drop_last=False,\\\n",
    "                    num_workers=num_workers, pin_memory=True, worker_init_fn=seed_worker,\\\n",
    "                    generator=torch.Generator().manual_seed(torch.initial_seed()), persistent_workers=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False,\\\n",
    "                    num_workers=num_workers, worker_init_fn=seed_worker,\\\n",
    "                    generator=torch.Generator().manual_seed(torch.initial_seed()), persistent_workers=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False,\\\n",
    "                    num_workers=num_workers, worker_init_fn=seed_worker,\\\n",
    "                    generator=torch.Generator().manual_seed(torch.initial_seed()), persistent_workers=True\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False,\\\n",
    "                    num_workers=num_workers, worker_init_fn=seed_worker,\\\n",
    "                    generator=torch.Generator().manual_seed(torch.initial_seed()), persistent_workers=True\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
